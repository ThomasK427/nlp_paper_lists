# A Paper List for the Representation Learning in Natural Language Processing

This is a paper list for representation learning in natural language processing.

## Paper List

- Tomas Mikolov et al. Efficient Estimation of Word Representations in Vector Space. ICLR workshop 2013. [[paper]][1] 

- Tomas Mikolov et al. Distributed Representations of Words and Phrases and their Compositionality. NIPS 2013. [[paper]][2]  

- Quoc V. Le, Tomas Mikolov. Distributed Representations of Sentences and Documents. ICLR 2014. [[paper]][3]  

- Jeffrey Pennington et al. GloVe: Global Vectors for Word Representation. EMNLP 2014. [[paper]][4]  

- Piotr Bojanowski et al. Enriching Word Vectors with Subword Information. TACL 2017. [[paper]][5]  

- Matthew E. Peters et al. Deep contextualized word representations. NAACL 2018. [[paper]][6]  

- Jeremy Howard, Sebastian Ruder. Universal Language Model Fine-tuning for Text Classification. ACL 2018. [[paper]][7]  

- Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL 2019 Best Long Paper. [[paper]][8]

- Zhouhan Lin et al. A Structured Self-attentive Sentence Embedding. ICLR 2017. [[paper]][9]

- Nal Kalchbrenner et al. A Convolutional Neural Network for Modelling Sentences. ACL 2014. [[paper]][10]

- John Hewitt, Christopher D. Manning. A Structural Probe for Finding Syntax in Word Representations. NAACL 2019. [[paper]][11]

- Zhilin Yang et al. XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv preprint 2019. [[paper]][12]

- Andy Coenen et al. Visualizing and Measuring the Geometry of BERT. arXiv preprint 2019. [[paper]][13]

- Ian Tenney et al. BERT Rediscovers the Classical NLP Pipeline. arXiv preprint 2019. [[paper]][14]

- Yifan Qiao et al. Understanding the Behaviors of BERT in Ranking. arXiv preprint 2019. [[paper]][15]

- Masahiro Kaneko, Mamoru Komachi. Multi-Head Multi-Layer Attention to Deep Language Representations for Grammatical Error Detection. arXiv preprint 2019. [[paper]][16]

- Matthew E. Peters et al. To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks. arXiv preprint 2019. [[paper]][17]

- Kevin Clark et al. What Does BERT Look At?An Analysis of BERTâ€™s Attention. ACL 2019 workshop. [[paper]][18]

- Ian Tenney et al. What do you learn from context? Probing for sentence structure in contextualized word representations. ICLR 2019. [[paper]][19]

- Ganesh Jawahar et al. What does BERT learn about the structure of language?. ACL 2019. [[paper]][20]

[1]: http://xxx.itp.ac.cn/abs/1301.3781
[2]: http://xxx.itp.ac.cn/abs/1310.4546
[3]: http://xxx.itp.ac.cn/abs/1405.4053
[4]: https://nlp.stanford.edu/pubs/glove.pdf
[5]: http://xxx.itp.ac.cn/abs/1607.04606
[6]: http://xxx.itp.ac.cn/abs/1802.05365?context=cs
[7]: http://xxx.itp.ac.cn/abs/1801.06146
[8]: http://xxx.itp.ac.cn/abs/1810.04805
[9]: http://xxx.itp.ac.cn/abs/1703.03130
[10]: http://www.aclweb.org/anthology/P14-1062
[11]: https://www.aclweb.org/anthology/N19-1419
[12]: http://xxx.itp.ac.cn/pdf/1906.08237.pdf
[13]: http://xxx.itp.ac.cn/pdf/1906.02715.pdf
[14]: http://xxx.itp.ac.cn/pdf/1905.05950.pdf
[15]: http://xxx.itp.ac.cn/pdf/1904.07531v1
[16]: http://xxx.itp.ac.cn/pdf/1904.07334v1
[17]: http://xxx.itp.ac.cn/pdf/1903.05987v2
[18]: http://xxx.itp.ac.cn/pdf/1906.04341.pdf
[19]: http://xxx.itp.ac.cn/pdf/1905.06316v1
[20]: https://hal.inria.fr/hal-02131630/document