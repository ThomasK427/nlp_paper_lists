# A Paper List for Machine Translation

This is a paper list for machine translation.

## Paper List

### seq2seq

- Ilya Sutskever et al. Sequence to Sequence Learning with Neural Networks. NIPS 2014. [[paper]][1]

- Jonas Gehring et al. Convolutional Sequence to Sequence Learning. ICML 2017. [[paper]][3]

- Ashish Vaswani et al. Attention Is All You Need. NIPS 2017. [[paper]][4]

### attention

- Dzmitry Bahdanau et al. Neural Machine Translation by Jointly Learning to Align and Translate. ICLR 2015. [[paper]][2]

- Minh-Thang Luong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015. [[paper]][8]

- Gongbo Tang et al. Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures. EMNLP 2018. [[paper]][5]

- Ofir Press, Noah A. Smith. You May Not Need Attention. arXiv preprint 2018. [[paper]][6]

- Tobias Domhan. How Much Attention Do You Need?A Granular Analysis of Neural Machine Translation Architectures. ACL 2018. [[paper]][10]

### Unsupervised

- Guillaume Lample et al.Phrase-Based & Neural Unsupervised Machine Translation. EMNLP 2018. [[paper]][7]

### other

- Barret Zoph, Kevin Knight. Multi-Source Neural Translation. NAACL 2016. [[paper]][9]


[1]:https://arxiv.org/abs/1409.3215
[2]:https://arxiv.org/abs/1409.0473v7
[3]:https://arxiv.org/abs/1705.03122
[4]:https://arxiv.org/abs/1706.03762
[5]:https://arxiv.org/abs/1808.08946v1
[6]:https://arxiv.org/abs/1810.13409
[7]:https://arxiv.org/abs/1804.07755
[8]:https://arxiv.org/abs/1508.04025v3
[9]:https://arxiv.org/abs/1601.00710
[10]:https://www.aclweb.org/anthology/P18-1167